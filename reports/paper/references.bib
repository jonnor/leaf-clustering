@Preamble{ " \newcommand{\noop}[1]{} " }

@misc{pnb-reguleringsplan,
  author={Justis- og beredskapsdepartementet},
  title={Vedtak av statlig reguleringsplat for politiets nasjonale beredskapsenter på Taraldrud i Ski kommune},
  year={2017},
  month={8},
  url="https://www.regjeringen.no/no/aktuelt/nasjonalt/id2567732/",
  ref={17/2261-28},
  __note={\newline\url{https://www.regjeringen.no/no/aktuelt/nasjonalt/id2567732/}},
}

@techreport{RieberPNBForbedredeTiltak,
  author={Dag Rieber},
  title={Vedlegg nr. 11-2 til reguleringsplan for Politiets nasjonale beredskapssenter, Støy fra skyte- og treningsaktiviteter},
  year={2017},
  url="https://www.regjeringen.no/no/aktuelt/nasjonalt/id2567732/",
  __note={\newline\url{https://www.regjeringen.no/no/aktuelt/nasjonalt/id2567732/}},
}

@techreport{T-1442/2016,
  title={T-1442/2016: Retningslinje for behandling av støy i arealplanlegging},
  year={2016},
  month={12},
  author={Klima- og miljødepartementet},
  institution={Klima- og miljødepartementet},
  url="https://www.regjeringen.no/no/dokumenter/retningslinje-for-behandling-av-stoy-i-arealplanlegging/id2526240/",
  ref={17/2261-28},
  __note={\newline\url{https://www.regjeringen.no/no/dokumenter/retningslinje-for-behandling-av-stoy-i-arealplanlegging/id2526240/}},
}

@techreport{Veileder-M128/2014,
  author={Miljødirektoratet},
  title={Veileder til retningslinje for behandling av støy i arealplanlegging ({{T-1442}})},
  year={2014},
  url="https://www.miljodirektoratet.no/publikasjoner/2014/februar-2014/veileder-til-retningslinje-for-behandling-av-stoy-i-arealplanlegging/",
  __note={\newline\url{https://www.miljodirektoratet.no/publikasjoner/2014/februar-2014/veileder-til-retningslinje-for-behandling-av-stoy-i-arealplanlegging/}},
}

@misc{ZoningPlanNorway,
    author    = "{{Norwegian Ministry of Local Government and Modernisation}}",
    title     = "Zoning plans, Plan og Bygningsloven",
    url       = "https://www.regjeringen.no/en/topics/plan-bygg-og-eiendom/plan--og-bygningsloven/planning/engelsk-test---planning-in-norway/engelsk-test----8-2/id710320/",
}

@misc{EuNoiseDirective,
    author    = "{{European Commision}}",
    title     = "EU directive 2002/49/EC: The assessment and management of environmental noise",
    url       = "http://ec.europa.eu/environment/noise/directive_en.htm",
}

@misc{IECSoundLevelMeters,
    title="IEC 61672-1 Sound Level Meters, part 1: Specifications",
    author="{{International Electrotechnical Commission}}",
    year={2013},
    url="https://standards.globalspec.com/std/1634276/iec-61672-1",
}
@misc{ANSISoundLevelMeters,
    title={ANSI S1.4-2014, Part 1: Specifications for Sound Level Meters},
    year={2014},
    author={Acoustical Society of America},
    url="https://webstore.ansi.org/Standards/ASA/ANSIASAS12014PartIEC616722013"
}

@misc{IECPersonalSoundExposureMeters,
    title="IEC 61252 Personal Sound Exposure Meters",
    author="{{International Electrotechnical Commission}}",
    year={2017},
    url="https://standards.globalspec.com/std/10153028/IEC%2061252",
}
@misc{IECOctaveBands,
    title="IEC 61260-1 Octave-band and fractional-octave-band filters, part 1: Specifications",
    author="{{International Electrotechnical Commission}}",
    year={2014},
    url="https://webstore.iec.ch/publication/5063",
}

@misc{ANSIOctaveBands,
    title={ANSI S1.11-2004: Octave-Band and Fractional-Octave-Band Analog and Digital Filters},
    year={2004},
    url={https://webstore.ansi.org/standards/asa/ansiasas1112004r2009}
}

@article{rabiner1986:HMM,
  title={An introduction to hidden Markov models},
  author={Rabiner, Lawrence and Juang, Biinghwang},
  journal={IEEE ASSP Magazine},
  volume={3},
  number={1},
  pages={4--16},
  year={1986},
  publisher={IEEE}
}


@article{gontier_efficient_2017:censecoder,
	title = {An Efficient Audio Coding Scheme for Quantitative and Qualitative Large Scale Acoustic Monitoring Using the Sensor Grid Approach},
	volume = {17},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/17/12/2758},
	doi = {10.3390/s17122758},
	pages = {2758},
	number = {12},
	year = 2017,
	journal = {Sensors},
	author = {Gontier, Félix and Lagrange, Mathieu and Aumond, Pierre and Can, Arnaud and Lavandier, Catherine},
	urldate = {2021-03-23},
	date = {2017-12},
	langid = {english},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {acoustic monitoring, audio encoding},
	file = {Full Text PDF:/home/jon/Zotero/storage/K8AC3XUY/Gontier et al. - 2017 - An Efficient Audio Coding Scheme for Quantitative .pdf:application/pdf;Snapshot:/home/jon/Zotero/storage/HPI3N5RA/htm.html:text/html},
}


@article{luo_wireless_2020,
	title = {Wireless Sensor Networks for Noise Measurement and Acoustic Event Recognitions in Urban Environments},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/20/7/2093},
	doi = {10.3390/s20072093},
	abstract = {Nowadays, urban noise emerges as a distinct threat to people\&rsquo;s physiological and psychological health. Previous works mainly focus on the measurement and mapping of the noise by using Wireless Acoustic Sensor Networks ({WASNs}) and further propose some methods that can effectively reduce the noise pollution in urban environments. In addition, the research on the combination of environmental noise measurement and acoustic events recognition are rapidly progressing. In a real-life application, there still exists the challenges on the hardware design with enough computational capacity, the reduction of data amount with a reasonable method, the acoustic recognition with {CNNs}, and the deployment for the long-term outdoor monitoring. In this paper, we develop a novel system that utilizes the {WASNs} to monitor the urban noise and recognize acoustic events with a high performance. Specifically, the proposed system mainly includes the following three stages: (1) We used multiple sensor nodes that are equipped with various hardware devices and performed with assorted signal processing methods to capture noise levels and audio data; (2) the Convolutional Neural Networks ({CNNs}) take such captured data as inputs and classify them into different labels such as car horn, shout, crash, explosion; (3) we design a monitoring platform to visualize noise maps, acoustic event information, and noise statistics. Most importantly, we consider how to design effective sensor nodes in terms of cost, data transmission, and outdoor deployment. Experimental results demonstrate that the proposed system can measure the urban noise and recognize acoustic events with a high performance in real-life scenarios.},
	pages = {2093},
	number = {7},
	journaltitle = {Sensors},
	author = {Luo, Liyan and Qin, Hongming and Song, Xiyu and Wang, Mei and Qiu, Hongbing and Zhou, Zou},
	urldate = {2021-03-22},
	date = {2020-01},
	langid = {english},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {acoustic events recognition, {CNNs}, noise measurement, real-life scenarios, {WASNs}},
	file = {Full Text PDF:/home/jon/Zotero/storage/P4CW3YK4/Luo et al. - 2020 - Wireless Sensor Networks for Noise Measurement and.pdf:application/pdf;Snapshot:/home/jon/Zotero/storage/8TNL6QNM/htm.html:text/html},
}


@article{maijala_environmental_2018,
	title = {Environmental noise monitoring using source classification in sensors},
	volume = {129},
	issn = {0003-682X},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X17307533},
	doi = {10.1016/j.apacoust.2017.08.006},
	pages = {258--267},
	journaltitle = {Applied Acoustics},
	shortjournal = {Applied Acoustics},
	author = {Maijala, Panu and Shuyang, Zhao and Heittola, Toni and Virtanen, Tuomas},
	urldate = {2021-03-23},
	date = {2018-01-01},
	langid = {english},
	keywords = {Acoustic pattern classification, Cloud service, Environmental noise monitoring, Wireless sensor network},
	file = {ScienceDirect Full Text PDF:/home/jon/Zotero/storage/ALKXGS4E/Maijala et al. - 2018 - Environmental noise monitoring using source classi.pdf:application/pdf;ScienceDirect Snapshot:/home/jon/Zotero/storage/QK7XZXA6/S0003682X17307533.html:text/html},
}

@article{nordby_environmental_2019,
	title = {Environmental sound classification on microcontrollers using Convolutional Neural Networks},
	rights = {Navngivelse-Ikkekommersiell-{DelPåSammeVilkår} 4.0 Internasjonal},
	url = {https://nmbu.brage.unit.no/nmbu-xmlui/handle/11250/2611624},
	author = {Nordby, Jon Opedal},
	urldate = {2021-03-23},
	date = {2019},
	note = {Accepted: 2019-08-29T11:42:30Z
Publisher: Norwegian University of Life Sciences, Ås},
	file = {Full Text PDF:/home/jon/Zotero/storage/FDBGYBV8/Nordby - 2019 - Environmental sound classification on microcontrol.pdf:application/pdf;Snapshot:/home/jon/Zotero/storage/ZBBRCHWZ/2611624.html:text/html},
}

@article{socoro_anomalous_2017,
	title = {An Anomalous Noise Events Detector for Dynamic Road Traffic Noise Mapping in Real-Life Urban and Suburban Environments},
	volume = {17},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/17/10/2323},
	doi = {10.3390/s17102323},
	pages = {2323},
	number = {10},
	journaltitle = {Sensors},
	author = {Socoró, Joan Claudi and Alías, Francesc and Alsina-Pagès, Rosa Ma},
	urldate = {2021-03-23},
	date = {2017-10},
	langid = {english},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {anomalous noise events, background noise, binary classification, dynamic noise mapping, real-life audio database, real-time acoustic event detection, road traffic noise, urban and suburban environments, wireless acoustic sensor network},
	file = {Full Text PDF:/home/jon/Zotero/storage/G33E4ZCF/Socoró et al. - 2017 - An Anomalous Noise Events Detector for Dynamic Roa.pdf:application/pdf;Snapshot:/home/jon/Zotero/storage/YUJHS5FY/2323.html:text/html},
}

@article{abdoli_end2end_2019,
	title = {End-to-end environmental sound classification using a 1D convolutional neural network},
	volume = {136},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304403},
	doi = {10.1016/j.eswa.2019.06.040},
	abstract = {In this paper, we present an end-to-end approach for environmental sound classification based on a 1D Convolution Neural Network ({CNN}) that learns a representation directly from the audio signal. Several convolutional layers are used to capture the signal’s fine time structure and learn diverse filters that are relevant to the classification task. The proposed approach can deal with audio signals of any length as it splits the signal into overlapped frames using a sliding window. Different architectures considering several input sizes are evaluated, including the initialization of the first convolutional layer with a Gammatone filterbank that models the human auditory filter response in the cochlea. The performance of the proposed end-to-end approach in classifying environmental sounds was assessed on the {UrbanSound}8k dataset and the experimental results have shown that it achieves 89\% of mean accuracy. Therefore, the proposed approach outperforms most of the state-of-the-art approaches that use handcrafted features or 2D representations as input. Moreover, the proposed approach outperforms all approaches that use raw audio signal as input to the classifier. Furthermore, the proposed approach has a small number of parameters compared to other architectures found in the literature, which reduces the amount of data required for training.},
	pages = {252--263},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Abdoli, Sajjad and Cardinal, Patrick and Lameiras Koerich, Alessandro},
	urldate = {2021-03-23},
	date = {2019-12-01},
	langid = {english},
	keywords = {Convolutional neural network, Deep learning, Environmental sound classification, Gammatone filterbank},
	file = {Submitted Version:/home/jon/Zotero/storage/EPGH3K9R/Abdoli et al. - 2019 - End-to-end environmental sound classification usin.pdf:application/pdf;ScienceDirect Snapshot:/home/jon/Zotero/storage/WJ8MLZPH/S0957417419304403.html:text/html},
}

@inproceedings{zhang_deep_2018,
	location = {Cham},
	title = {Deep Convolutional Neural Network with Mixup for Environmental Sound Classification},
	isbn = {978-3-030-03335-4},
	doi = {10.1007/978-3-030-03335-4_31},
	series = {Lecture Notes in Computer Science},
	abstract = {Environmental sound classification ({ESC}) is an important and challenging problem. In contrast to speech, sound events have noise-like nature and may be produced by a wide variety of sources. In this paper, we propose to use a novel deep convolutional neural network for {ESC} tasks. Our network architecture uses stacked convolutional and pooling layers to extract high-level feature representations from spectrogram-like features. Furthermore, we apply mixup to {ESC} tasks and explore its impacts on classification performance and feature distribution. Experiments were conducted on {UrbanSound}8K, {ESC}-50 and {ESC}-10 datasets. Our experimental results demonstrated that our {ESC} system has achieved the state-of-the-art performance (83.7\%\%{\textbackslash}\%) on {UrbanSound}8K and competitive performance on {ESC}-50 and {ESC}-10.},
	pages = {356--367},
	booktitle = {Pattern Recognition and Computer Vision},
	publisher = {Springer International Publishing},
	author = {Zhang, Zhichao and Xu, Shugong and Cao, Shan and Zhang, Shunqing},
	editor = {Lai, Jian-Huang and Liu, Cheng-Lin and Chen, Xilin and Zhou, Jie and Tan, Tieniu and Zheng, Nanning and Zha, Hongbin},
	date = {2018},
	langid = {english},
	keywords = {Convolutional neural network, Environmental sound classification, Mixup},
	file = {Submitted Version:/home/jon/Zotero/storage/785C4RFI/Zhang et al. - 2018 - Deep Convolutional Neural Network with Mixup for E.pdf:application/pdf},
}

@inproceedings{cramer_look_2019:OpenL3,
	title = {Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings},
	doi = {10.1109/ICASSP.2019.8682475},
	shorttitle = {Look, Listen, and Learn More},
	eventtitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {3852--3856},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Cramer, J. and Wu, H. and Salamon, J. and Bello, J. P.},
	date = {2019-05},
	note = {{ISSN}: 2379-190X},
	keywords = {deep learning, learning (artificial intelligence), Data models, Spectrogram, Training, Audio classification, audio collections, audio signal processing, audio-informed choices, audio-visual correspondence, Computational modeling, deep audio embeddings, downstream audio classification tasks, downstream audio classifiers, labeled datasets, machine listening, net design choices, net embedding model, self-supervised learning, signal classification, {SoundNet} embeddings, Task analysis, Training data, transfer learning, {VGGish} embeddings, Videos},
	file = {IEEE Xplore Abstract Record:/home/jon/Zotero/storage/EY3AFA85/8682475.html:text/html},
}

@inproceedings{piczak2015dataset:esc50,
  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},
  author = {Piczak, Karol J.},
  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},
  date = {2015-10-13},
  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},
  doi = {10.1145/2733373.2806390},
  location = {{Brisbane, Australia}},
  isbn = {978-1-4503-3459-4},
  publisher = {{ACM Press}},
  pages = {1015--1018}
}


@inproceedings{salamon_dataset_2014:urbansound8k,
	location = {New York, {NY}, {USA}},
	title = {A Dataset and Taxonomy for Urban Sound Research},
	isbn = {978-1-4503-3063-3},
	url = {https://doi.org/10.1145/2647868.2655045},
	doi = {10.1145/2647868.2655045},
	series = {{MM} '14},
	abstract = {Automatic urban sound classification is a growing area of research with applications in multimedia retrieval and urban informatics. In this paper we identify two main barriers to research in this area - the lack of a common taxonomy and the scarceness of large, real-world, annotated data. To address these issues we present a taxonomy of urban sounds and a new dataset, {UrbanSound}, containing 27 hours of audio with 18.5 hours of annotated sound event occurrences across 10 sound classes. The challenges presented by the new dataset are studied through a series of experiments using a baseline classification system.},
	pages = {1041--1044},
	booktitle = {Proceedings of the 22nd {ACM} international conference on Multimedia},
	publisher = {Association for Computing Machinery},
	author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},
	urldate = {2021-03-22},
	date = {2014-11-03},
	keywords = {classification, dataset, taxonomy, urban sound},
	file = {Submitted Version:/home/jon/Zotero/storage/V68YFI5L/Salamon et al. - 2014 - A Dataset and Taxonomy for Urban Sound Research.pdf:application/pdf},
}

@misc{mark_cartwright_sonyc_2019,
	title = {{SONYC} Urban Sound Tagging ({SONYC}-{UST}): a multilabel dataset from an urban acoustic sensor network},
	url = {https://zenodo.org/record/3233082},
	shorttitle = {{SONYC} Urban Sound Tagging ({SONYC}-{UST})},
	publisher = {Zenodo},
	author = {Mark Cartwright and Ana Elisa Mendez Mendez and Graham Dove and Jason Cramer and Vincent Lostanlen and Ho-Hsiang Wu and Justin Salamon and Oded Nov and Juan Pablo Bello},
	urldate = {2021-03-22},
	date = {2019-05-28},
	doi = {10.5281/zenodo.3233082},
	note = {type: dataset},
	keywords = {machine listening, noise pollution, urban sound},
	file = {Zenodo Snapshot:/home/jon/Zotero/storage/PJABJ39D/3233082.html:text/html},
}

@article{cartwright_sonyc-ust-v2_2020,
	title = {{SONYC}-{UST}-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context},
	url = {http://arxiv.org/abs/2009.05188},
	shorttitle = {{SONYC}-{UST}-V2},
	abstract = {We present {SONYC}-{UST}-V2, a dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. {SONYC}-{UST}-V2 consists of 18510 audio recordings from the "Sounds of New York City" ({SONYC}) acoustic sensor network, including the timestamp of audio acquisition and location of the sensor. The dataset contains annotations by volunteers from the Zooniverse citizen science platform, as well as a two-stage verification with our team. In this article, we describe our data collection procedure and propose evaluation metrics for multilabel classification of urban sound tags. We report the results of a simple baseline model that exploits spatiotemporal information.},
	journaltitle = {{arXiv}:2009.05188 [cs, eess]},
	author = {Cartwright, Mark and Cramer, Jason and Mendez, Ana Elisa Mendez and Wang, Yu and Wu, Ho-Hsiang and Lostanlen, Vincent and Fuentes, Magdalena and Dove, Graham and Mydlarz, Charlie and Salamon, Justin and Nov, Oded and Bello, Juan Pablo},
	urldate = {2021-03-22},
	date = {2020-09-10},
	eprinttype = {arxiv},
	eprint = {2009.05188},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/jon/Zotero/storage/C2HP7SIZ/Cartwright et al. - 2020 - SONYC-UST-V2 An Urban Sound Tagging Dataset with .pdf:application/pdf;arXiv.org Snapshot:/home/jon/Zotero/storage/SYELFUAT/2009.html:text/html},
}



@inproceedings{valenzise_scream_2007,
	title = {Scream and gunshot detection and localization for audio-surveillance systems},
	doi = {10.1109/AVSS.2007.4425280},
	abstract = {This paper describes an audio-based video surveillance system which automatically detects anomalous audio events in a public square, such as screams or gunshots, and localizes the position of the acoustic source, in such a way that a video-camera is steered consequently. The system employs two parallel {GMM} classifiers for discriminating screams from noise and gunshots from noise, respectively. Each classifier is trained using different features, chosen from a set of both conventional and innovative audio features. The location of the acoustic source which has produced the sound event is estimated by computing the time difference of arrivals of the signal at a microphone array and using linear-correction least square localization algorithm. Experimental results show that our system can detect events with a precision of 93\% at a false rejection rate of 5\% when the {SNR} is 10dB, while the source direction can be estimated with a precision of one degree. A real-time implementation of the system is going to be installed in a public square of Milan.},
	eventtitle = {2007 {IEEE} Conference on Advanced Video and Signal Based Surveillance},
	pages = {21--26},
	booktitle = {2007 {IEEE} Conference on Advanced Video and Signal Based Surveillance},
	author = {Valenzise, G. and Gerosa, L. and Tagliasacchi, M. and Antonacci, F. and Sarti, A.},
	date = {2007-09},
	keywords = {Acoustic arrays, Acoustic noise, acoustic signal detection, Acoustic signal detection, acoustic source, anomalous audio events, audio features, audio signal processing, audio-based video surveillance, Event detection, gunshot detection, Gunshot detection systems, image classification, Least squares approximation, linear-correction localization algorithm, localization, microphone array, Microphone arrays, Real time systems, scream audiosurveillance systems, scream detection, signal arrival, signal classifier, Time difference of arrival, video surveillance, Video surveillance, video-camera},
	file = {IEEE Xplore Abstract Record:/home/jon/Zotero/storage/N99ZWZHY/4425280.html:text/html},
}

@inproceedings{maher_modeling_2006,
	title = {Modeling and Signal Processing of Acoustic Gunshot Recordings},
	doi = {10.1109/DSPWS.2006.265386},
	abstract = {Audio recordings of gunshots can provide information about the gun location with respect to the microphone(s), the speed and trajectory of the projectile, and in some cases the type of firearm and ammunition. Recordings obtained under carefully controlled conditions can be well-modeled by geometrical acoustics. Special acoustic processing systems for real time gunshot detection and localization are used by the military and law enforcement agencies for sniper detection. Forensic analysis of audio recordings is also used to provide evidence in criminal and civil cases. This paper reviews the distinctive features and limitations of acoustic gunshot analysis using {DSP} techniques},
	eventtitle = {2006 {IEEE} 12th Digital Signal Processing Workshop 4th {IEEE} Signal Processing Education Workshop},
	pages = {257--261},
	booktitle = {2006 {IEEE} 12th Digital Signal Processing Workshop 4th {IEEE} Signal Processing Education Workshop},
	author = {Maher, R. C.},
	date = {2006-09},
	keywords = {Acoustic propagation, acoustic signal detection, acoustic signal processing, Acoustic signal processing, Acoustic waves, audio recording, Audio recording, digital signal processing, {DSP} technique, Electric shock, forensic analysis, geometrical acoustics, gunshot audio recording, Gunshot detection systems, gunshots, microphone, microphones, Microphones, military-law enforcement agency, projectile trajectory, projectiles, Projectiles, real time gunshot detection, Reflection, shock wave acoustics, Shock waves, sniper detection, weapons},
	file = {Full Text:/home/jon/Zotero/storage/9YS4MKLU/Maher - 2006 - Modeling and Signal Processing of Acoustic Gunshot.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jon/Zotero/storage/PCEJFU2P/4041069.html:text/html},
}

@article{salamon_deep_2017:SBCNN,
	title = {Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification},
	volume = {24},
	issn = {1558-2361},
	doi = {10.1109/LSP.2017.2657381},
	pages = {279--283},
	number = {3},
	journaltitle = {{IEEE} Signal Processing Letters},
	author = {Salamon, J. and Bello, J. P.},
	date = {2017-03},
	note = {Conference Name: {IEEE} Signal Processing Letters},
	keywords = {audio data augmentation, audio signal processing, augmented training set, class-conditional data augmentation, Convolution, Data models, deep convolutional neural networks, Deep convolutional neural networks ({CNNs}), deep learning, discriminative spectro-temporal patterns, environmental sound classification, high-capacity models, neural nets, Neural networks, Shape, signal classification, Time-frequency analysis, Training, Training data, urban sound dataset},
	file = {Submitted Version:/home/jon/Zotero/storage/N8G7IDHK/Salamon and Bello - 2017 - Deep Convolutional Neural Networks and Data Augmen.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jon/Zotero/storage/2FW3ZLEI/7829341.html:text/html},
}


@inproceedings{salamon_scaper_2017,
	title = {Scaper: A library for soundscape synthesis and augmentation},
	doi = {10.1109/WASPAA.2017.8170052},
	shorttitle = {Scaper},
	abstract = {Sound event detection ({SED}) in environmental recordings is a key topic of research in machine listening, with applications in noise monitoring for smart cities, self-driving cars, surveillance, bioa-coustic monitoring, and indexing of large multimedia collections. Developing new solutions for {SED} often relies on the availability of strongly labeled audio recordings, where the annotation includes the onset, offset and source of every event. Generating such precise annotations manually is very time consuming, and as a result existing datasets for {SED} with strong labels are scarce and limited in size. To address this issue, we present Scaper, an open-source library for soundscape synthesis and augmentation. Given a collection of iso-lated sound events, Scaper acts as a high-level sequencer that can generate multiple soundscapes from a single, probabilistically defined, “specification”. To increase the variability of the output, Scaper supports the application of audio transformations such as pitch shifting and time stretching individually to every event. To illustrate the potential of the library, we generate a dataset of 10,000 sound-scapes and use it to compare the performance of two state-of-the-art algorithms, including a breakdown by soundscape characteristics. We also describe how Scaper was used to generate audio stimuli for an audio labeling crowdsourcing experiment, and conclude with a discussion of Scaper's limitations and potential applications.},
	eventtitle = {2017 {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics ({WASPAA})},
	pages = {344--348},
	booktitle = {2017 {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics ({WASPAA})},
	author = {Salamon, J. and {MacConnell}, D. and Cartwright, M. and Li, P. and Bello, J. P.},
	date = {2017-10},
	note = {{ISSN}: 1947-1629},
	keywords = {Acoustics, Data models, Libraries, Monitoring, Probabilistic logic, Signal to noise ratio, sound event detection, Soundscape, synthesis, Training},
	file = {IEEE Xplore Abstract Record:/home/jon/Zotero/storage/U49FDIS3/8170052.html:text/html},
}



@article{mesaros_sound_2019,
	title = {Sound Event Detection in the {DCASE} 2017 Challenge},
	volume = {27},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2019.2907016},
	abstract = {Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events ({DCASE}) contained several tasks involving sound event detection in different setups. {DCASE} 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly labeled data were available for training. In this paper, we present three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency-based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure to perform statistical analysis of the challenge results. The analysis indicates that while the 95\% confidence intervals for many systems overlap, there are significant differences in performance between the top systems and the baseline for all tasks.},
	pages = {992--1006},
	number = {6},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Mesaros, Annamaria and Diment, Aleksandr and Elizalde, Benjamin and Heittola, Toni and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas},
	date = {2019-06},
	year = {2019},
	note = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	keywords = {Training, Task analysis, Hidden Markov models, Acoustics, Speech processing, Event detection, confidence intervals, Glass, jackknife estimates, pattern recognition, Sound event detection, weak labels},
	file = {Submitted Version:/home/jon/Zotero/storage/4ELI8XLE/Mesaros et al. - 2019 - Sound Event Detection in the DCASE 2017 Challenge.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jon/Zotero/storage/M9V85PRD/8673582.html:text/html},
}

@inproceedings{lafay_sound_2017,
	title = {Sound event detection in synthetic audio: Analysis of the DCASE 2016 task results},
	doi = {10.1109/WASPAA.2017.8169985},
	shorttitle = {Sound event detection in synthetic audio},
	abstract = {As part of the 2016 public evaluation challenge on Detection and Classification of Acoustic Scenes and Events ({DCASE} 2016), the second task focused on evaluating sound event detection systems using synthetic mixtures of office sounds. This task, which follows the `Event Detection-Office Synthetic' task of {DCASE} 2013, studies the behaviour of tested algorithms when facing controlled levels of audio complexity with respect to background noise and polyphony/density, with the added benefit of a very accurate ground truth. This paper presents the task formulation, evaluation metrics, submitted systems, and provides a statistical analysis of the results achieved, with respect to various aspects of the evaluation dataset.},
	eventtitle = {2017 {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics ({WASPAA})},
	pages = {11--15},
	booktitle = {2017 {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics ({WASPAA})},
	author = {Lafay, Grégoire and Benetos, Emmanouil and Lagrange, Mathieu},
	date = {2017-10},
	note = {{ISSN}: 1947-1629},
	keywords = {Training, Acoustics, Event detection, Sound event detection, acoustic scene analysis, Analysis of variance, {DCASE}, experimental validation, Image analysis, Measurement, sound scene analysis},
	file = {Submitted Version:/home/jon/Zotero/storage/LPCVNH3I/Lafay et al. - 2017 - Sound event detection in synthetic audio Analysis.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jon/Zotero/storage/CL7SRQTM/8169985.html:text/html},
}


@article{stowell_detection_2015,
	title = {Detection and Classification of Acoustic Scenes and Events},
	volume = {17},
	issn = {1941-0077},
	doi = {10.1109/TMM.2015.2428998},
	abstract = {For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the {IEEE} Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events ({DCASE}). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.},
	pages = {1733--1746},
	number = {10},
	journaltitle = {{IEEE} Transactions on Multimedia},
	author = {Stowell, Dan and Giannoulis, Dimitrios and Benetos, Emmanouil and Lagrange, Mathieu and Plumbley, Mark D.},
	date = {2015-10},
	note = {Conference Name: {IEEE} Transactions on Multimedia},
	keywords = {Audio databases, event detection, Event detection, Licenses, machine intelligence, Microphones, Music, pattern recognition, Speech, Speech recognition},
	file = {IEEE Xplore Full Text PDF:/home/jon/Zotero/storage/XUYB5CFT/Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jon/Zotero/storage/2WXZZZZ2/7100934.html:text/html},
}

@Article{mesaros_sed_eval,
AUTHOR = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
TITLE = {Metrics for Polyphonic Sound Event Detection},
JOURNAL = {Applied Sciences},
VOLUME = {6},
YEAR = {2016},
NUMBER = {6},
ARTICLE-NUMBER = {162},
URL = {https://www.mdpi.com/2076-3417/6/6/162},
ISSN = {2076-3417},
ABSTRACT = {This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.},
DOI = {10.3390/app6060162}
}


@article{qiuqiang_kong_panns_nodate,
	title = {{PANNs}: {Large}-{Scale} {Pretrained} {Audio} {Neural} {Networks} for {Audio} {Pattern} {Recognition}},
	author = {Qiuqiang Kong and Yin Cao, Member and Turab Iqbal and Yuxuan Wang and Wenwu Wang and Mark D. Plumbley},
}

@article{justin_salamon_juan_pablo_bello_deep_2016,
	title = {Deep {Convolutional} {Neural} {Networks} and {Data} {Augmentation} for {Environmental} {Sound} {Classification}},
	author = {Justin Salamon, Juan Pablo Bello},
	month = nov,
	year = {2016},
}




@techreport{jonnordby2021,
	title = {AUTOMATIC DETECTION OF NOISE EVENTS AT SHOOTING RANGE USING MACHINE LEARNING},
	url = {https://arxiv.org/abs/2107.11453},
	abstract = {Outdoor shooting ranges are subject to noise regulations from local and national authorities. Restrictions found in these regulations may include limits on times of activities, the overall number of noise events, as well as limits on number of events depending on the class of noise or activity. A noise monitoring system may be used to track overall sound levels, but rarely provide the ability to detect activity or count the number of events, required to compare directly with such regulations. This work investigates the feasibility and performance of an automatic detection system to count noise events. An empirical evaluation was done by collecting data at a newly constructed shooting range and training facility. The data includes tests of multiple weapon configurations from small firearms to high caliber rifles and explosives, at multiple source positions, and collected on multiple different days. Several alternative machine learning models are tested, using as inputs time-series of standard acoustic indicators such as A-weighted sound levels and 1/3 octave spectrogram, and classifiers such as Logistic Regression and Convolutional Neural Networks. Performance for the various alternatives are reported in terms of the False Positive Rate and False Negative Rate. The detection performance was found to be satisfactory for use in automatic logging of time-periods with training activity.},
	author = {Nordby, Jon and Rieber, Dag and Nemazi, Fabian},
	langid = {english},
	year = 2021,
	keywords = {Convolutional neural network, Deep learning, Environmental sound classification}
}

@article{chicco2021siamese,
  title={Siamese neural networks: An overview},
  author={Chicco, Davide},
  journal={Artificial Neural Networks},
  pages={73--94},
  year={2021},
  publisher={Springer}
}


@mastersthesis{fabiannemazi,
	title = {Determining the origin of impulsive noise events
using wireless sound sensors},
	author = {Nemazi, Fabian},
	year = 2021,
	month = 6,
	school={Norwegian University of Life Sciences},
	note = {
Publisher: Norwegian University of Life Science
}
}
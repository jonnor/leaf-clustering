
% Arxiv styling
% Based on https://github.com/kourgeorge/arxiv-style
\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[strings]{underscore} % make sure _ in strings like \texttt works 
\usepackage{floatrow}
%\usepackage{natbib}
\usepackage{doi}
\usepackage{graphicx}
\usepackage{subcaption}

% Document
\begin{document}

%% Custom commands
% Inline reference to a hyperparameter.
% Often contains underscores, which must be detokinized
\newcommand{\hyperparam}[1]{\texttt{\detokenize{#1}}}

%% Title
\title{(preliminary title) Model compression for Random Forests using leaf clustering}

%% Author info
\author{
    Jon Nordby \\
	Soundsensing AS \\
	\texttt{jon@soundsensing.no} \\	
}

% Remove the date
\date{}

\maketitle
\renewcommand{\abstractname}{\vspace{-\baselineskip}} % erase the space between authors and abstract

%% Abstract
\begin{abstract}	\noindent
The Random Forest algorithm has been very successful, and continue to be a powerful choice on low-power embedded systems with small amounts of memory and compute.
Current size-optimized implementations of Random Forest inference tend to use hard majority voting, where each decision tree returns the most probable class, instead of the class proportions (probabilities).
We show that while compact, the hard majority voting rule can lead to a large drop in predictive performance on some datasets.
To enable a better tradeoff between predictive performance and model size,
we propose two methods to compress the model:
using leaf quantization with deduplication, or leaf clustering with K-means.
By evaluating using 60 classification datasets from the OpenML CC18 benchmark suite,
we find that only a few class probabilities need to be stored to retain good performance, with small increase in model size over hard majority voting.
When combined with 16-bit integer quantization of the input features, reducing the number of trees, the method reached average model compression of XXX, when staying within XXX percentage point performance drop of the uncompressed baseline.
The method is implemented in the open-source TinyML library emlearn,
and demonstrated running on microcontrollers for Human Activity recognition tasks.

% Keywords. Maximum 5
\noindent \textbf{Keywords}: Random Forest, TinyML, model compression, tree-based ensemble

\end{abstract}

\section{Introduction}

Big picture motivation.
TinyML use-cases.
TinyML setting. On device inference. Low power. Wireless sensing.

Importance of small models.
Microcontroller. Limited program storage (FLASH) and memory (SRAM).

\noindent

\newpage
\section{Background}

Random Forest model was originally introduced by \cite{random_forest_breiman2001}

Decision tree ensembles have several properties that make them attractive
for resource-constrained environments.
Fast inference
Easy to implement with integer-only arithmetic.
High predictive performance with minimal hyperparameter tuning.
Works well with small and medium sized datasets.

Success cases for Random Forest in TinyML.
Human Activity Recognition. \cite{elsts_are_2021}
Animal behavior. \cite{tatler_high_2018} \cite{kleanthous_feature_2020} \cite{tran_iot-based_2022}

% REGRESSION, not classification Battery monitoring. \cite{mawonou_state--health_2021}


Structure of a random forest.

Logically, a decision tree ensemble consists of a set of binary decision trees.
Each tree has decision nodes that evaluate a set of if-else conditions,
comparing one of the features of the input data to a learned threshold value. 
At the leaves of the trees are the classification predictions for that tree.

The leaves can either be a class index, in which case the ensemble prediction is done by counting the number of votes and outputting the majority.

Probability average. Soft.
% TODO: have some references to proportions, when it was introduced?


% TODO: illustration of logical tree structure

There are multiple ways of representing the decision tree ensemble at inference time.
% TODO: some references to other approaches
% As implemented in scikit-learn \cite{scikit-learn} RandomForestClassifier.

Here we use an approach consisting of two tables:
a table of decision nodes, plus a table of leaf nodes. 

Each decision node consists of feature index, threshold value, left child index and right child index.
The left/right indices can either be an offset to a new decision node, or an offset into the table of leaf nodes (encoded as negative values).
This memory layout is illustrated in Figure XXX: FIXME.
% TODO: illustration of the storage, of decision nodes and leaves

Each entry in the leaf node table has XXXX class proportions.

Class probabilities. 32 bit floating point.
$leaf_size: n_leaves * n_classes * 4 bytes$


If there are no restrictions on tree depth,
all leaves end up only containing samples from a single class.
In this circumstance, storing only the class index (instead of class proportions) preserves all the information in the tree.
However, when there are restrictions on tree depth
(for example with scikit-learn hyperparameters \hyperparam{max_depth}, \hyperparam{min_samples_leaf}, \hyperparam{min_samples_split}, et.c.)
the leaves will containe a mix of class proportions.

In resource constrained environments, this is the more realistic case.


Ensemble voting rule:
\begin{verbatim}
probabilities[n_trees, n_classes]
for tree in forest:
   tree_probabilities = predict_tree(tree)
   probabilities[tree] = tree_probabilities
proba = mean(probabilities, axis=1)
\end{verbatim}


Hard majority. argmax. With leaf-deduplication.
Using 1 byte for the class indices, allows up to 255 classes.

$leaf_size = n_classes * 1 * 1 byte$

Voting rule:
\begin{verbatim}
votes[n_classes] = {0,...}
for tree in forest:
   class_index = predict_tree(tree)
   votes[class_index] += 1
out = argmax(votes)
proba = votes / n_classes
\end{verbatim}

As implemented in emlearn 0.9.0 (2019) \cite{emlearn}.

Proposed. Use soft voting rule.
Reduce $n_leaves$ using clustering and reduce bitdepth using quantization.


\subsection{Related work}

Optimization techniques for decision trees and decision tree ensembles (such as Random Forest) has been studied extensively over the last decades, including in a low-power and low-compute settings.

Integer quantization of features.
FPGA/hardware related. FIXME: references

SIMD tree evaluation.
TODO: cite QuickScorer/RAPIDSCORER, Fast Inference of Tree Ensembles on ARM Devices

Optimization for microcontrollers \cite{tabanelli_optimizing_2022}
Feature selection. \cite{uddin_guided_2015}, \cite{elsts_energy-efficient_2020}
Early stopping. \cite{daghero_low-overhead_2022} \cite{daghero_adaptive_2021}
Parallelization.
Cache-awareness.

Software tools for deploying RF to TinyML. Open-source
% TODO: cite m2cgen, emlearn, etc

\section{Methods}

What we cover: Feature quantization. Leaf quantization. Leaf clustering.

\subsection{Comparison of leaf compression strategies on OpenML CC18}


A subset of datasets from the OpenML CC18 benchmarking suite\cite{oml-benchmarking-suites} were used as a basis for the evaluation. 
To avoid needing data imputation, the datasets with missing values were dropped,
and to avoid datasets which require substantial feature selection or engineering, datasets more than FIXME:XX features were dropped.
The selection removed FIXME:XXX datasets, leaving a total of FIXME:XXX datasets. The complete list can be found in the results.
% MAYBE: list exact identifiers that which was eliminated?

Numerical variables are scaled using RobustScaler, and categorical variables are encoded using OrdinalEncoder.

Train a general model. Using scikit-learn. \cite{scikit-learn}

Apply compression strategy to trained model. Re-running predictions on test set.
Cross validation using k-fold, with k=5.

Metric. AUC ROC one-versus-all.



All experiments were done with int16 as feature representation,
which reduces the size of decision nodes considerably compared to float32, while still allowing a sufficient range to give equivalent results on most datasets/tasks.
% FIXME: references to existing papers
We also found that int16 features resulted in the same performance as float32 on the selected datasets (not shown).
The conversion from original feature data to 16-bit integers was done using robust min-max scaling between 0.01 percentile and 99.9 percentile.


An overview of the different model compression strategies can be seen in Table \ref{fig:leaf_uniqueness}. 
The Baseline configuration uses the default scikit-learn hyper-parameters (as of version FIXME), 100 trees (\texttt{n_estimators}) and no limit to tree depth (\texttt{min_samples_leaf=1}).
All other configurations used 10 trees (more representative for models for resource constrained targets), and a range of tree depths were considered by trying \texttt{min_samples_leaf} values between 1,2,4,8,16,32,64,128.
For Quantize we use between 1-8 bit quantization, and for ClusterQ8 consider 1,2,4,8,16,32 clusters per class.

%NOTE: clustering will probably save more in percent, the more leaves are used.
%Increases both with the number of trees, and the effective depth
%- but total model sizes in excess of 100 kB is less relevant for the typical usecase

\begin{table}[h]
\centering
\begin{tabular}{llllll}
\hline
\textbf{Strategy} & \textbf{Trees} & \textbf{Leaf representation} & \textbf{Deduplication} & \textbf{Clustering} & \textbf{Options} \\
\hline
Baseline & 100 & float32 & No & No  \\
Reduced & 10 & float32 & No & No \\
Majority & 10 & none & Yes & No \\
Deduplicate & 10 & float32 & Yes & No \\
Quantize & 10 & uint8 & Yes & No & 1-8 bit quantization \\
ClusterQ8 & 10 & uint8 & Yes & Yes & 1,2,4,8,16,32 clusters per class \\
% ClusterQ4 & 10 & 4+4 bits in uint8 & Yes & Yes \\
\hline
\end{tabular}
\caption{Experimental configurations}
\label{tab:models}
\end{table}


\subsection{Evaluation on hardware for Human Activity Recognition}

? Demonstrator of performance on one or few TinyML tasks/datasets.
HAR: PAMAP. UCI DSADS. OPPORTUNITY ?
F1 scores. 

Feature process. TODO: decide. MiniRocket ?? Any pre-selection?

Comparisons with other frameworks. Leaf cluster (emlearn), m2cgen, micromlgen

Hardware architectures.
RP2040 ARM Cortex M0+ (no FPU). 
Nordic NRF52840 ARM Cortex M4F
ESP32 (FPU).

TODO: visualization. Panel over HW, dataset of scatterplots with model size and inference time


\footnote{The code for reproducing the experiments can be found at https://github.com/jonnor/leaf-quantization}
% FIXME: double check link before submission

\newpage
\section{Results}

0. Baseline for comparison.
TODO: show a plot of the baseline performance?

1. Performance with 16bit and 8 bit feature quantization, vs floating point.
Without any changes to the leaves.
Practically no degradation in performance.
TODO: add some supporting references to 16 bits being enough. Both in general, and specific to decision trees / random forest

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=0.98\linewidth]{reports/figures/int16-vs-float.png}
  \end{center}
  \caption{ Performance of different feature representations, relative to the strong baseline model. Computed across all datasets and hyperparameters. 16-bit integer quantized features shows practically identical performance to full precision floating point. }
  \label{fig:int16_vs_float}
\end{figure}

\begin{figure}[h!]
\begin{center}
  \includegraphics[width=0.98\linewidth]{reports/figures/hyperparam-perfdrop-trees-strategies.png}
  \end{center}
  \caption{ Performance relative to the strong baseline model, averaged over all datasets. With no restriction on tree growth (\hyperparam{min_samples_leaf=1}, leftmost side), majority voting and soft voting is identical. When tree growth is restricted, majority voting performs worse compared to soft voting, and this gets worse with lower number of trees. Soft voting benefits from tree growth restrictions, especially with lower number of trees. }
  \label{fig:hyperparameter_strategies}
\end{figure}


As the trees are more restricted, the uniqueness of leaf nodes goes up. 
See Figure \ref{fig:leaf_uniqueness}).
This happens because the algorithm is force to stop splitting a node before reaching a perfectly pure node (consisting of samples from a single class), and instead is left with a leaf consisting of a data-dependent proportion.
This happens independently of the exact hyperparameter used to limit tree growth.
% CLAIM. Not yet verified for all possibilities. Just min_samples_leaf and max_depth

% TODO: run experiments for all hyperparameters restricting tree depth
% But probably no need to include all figures in the paper

Since unique leaves cannot be deduplicated and must be stored individually, this leads to a larger proportion of the total model size taken up by leaves.
This can be seen in Figure \ref{fig:leaf_proportions}).
This leads to a potential for saving space by optimizing how leaves are stored. 

TODO: also show \hyperparam{max_depth}  

FIXME: Leaf size used. Should be 32-bit floating point for leaf probabilities.
And int16 features?

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \includegraphics[width=2in]{reports/figures/leaf-uniqueness.png}
  \caption{ More unique leaf values are produced when there are restrictions on tree growth. }
  \label{fig:leaf_uniqueness}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=2in]{reports/figures/leaf-proportion.png}
  \caption{The proportion of model size used by leaf nodes make up changes as restriction on tree growth. TODO: write more }
  \label{fig:leaf_proportions}
\end{minipage}
\end{figure}

TODO: results with leaf clustering (and quantization) enabled
Thesis: is Pareto optimal in terms of predictive performance vs model size, when compared to majority and full soft voting




\subsection{Tests on microcontroller hardware}

REAL WORLD TEST.
? Would have to make some illustrations of this.
For example on HAR task/datasets, that on relevant microcontrollers, memory is problem before execution time. Need also to account for feature extraction time, which dominates.
Typical workflow. Set hard constraints on model size. Optimize for maximal predictive performance. Say 1kB, 10kB, 100kB - or maybe steps of 2x

VISUALIZATION. Model size vs predictive performance. And inference time vs predictive performance. Multiple model sizes. One line per framework, linked datapoints. Demonstrate Pareto optimality



%% DISCUSSION
\section{Discussion}

? claim. Model size is the typical main constraint for RF in TinyML settings.
Execution time (and power consumption) is less of a problem.

\subsection{Further work}

Here we only considered the use of leaf quantization and leaf clustering for classification, however we suspect that these methods would also be beneficial for  regression tasks.
The methods may also be useful for gradient-boosted trees (such as XGBoost, LightGBM, etc).

%\newpage
\section{Conclusions}


\section*{Acknowledgements}
\noindent

We would like to thank Martin Stensg√•rd for multiple insightful and productive discussions about low-level optimization of decision tree inference on microcontroller CPU arcitectures. 

Soundsensing has in the period 2022-2025 received funding from Research Council of Norway as part of the project EARONEDGE\cite{earonedge_nfr}.

% References
\newpage
\bibliographystyle{unsrt}
\bibliography{references.bib} 

\end{document}

